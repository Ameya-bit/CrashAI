# -*- coding: utf-8 -*-
"""CrashAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8WsyPbp5dcMeb1nLQ2s-elk2pG8PAKf
"""

import pandas as pd

# Replace 'your_file.csv' with the path to your CSV file
df = pd.read_csv('CRASH_2024.csv')

# Preview the first 5 rows
df.head()

crn_to_check = 2024011147

if crn_to_check in df['CRN'].values:
    print(f"CRN {crn_to_check} exists in the dataset ✅")
else:
    print(f"CRN {crn_to_check} does NOT exist in the dataset ❌")

import pandas as pd
from functools import reduce

# ---------------------------
# Step 1: Read all datasets
# ---------------------------
crash_df = pd.read_csv('CRASH_2024.csv')
vehicle_df = pd.read_csv('VEHICLE_2024.csv')
commveh_df = pd.read_csv('COMMVEH_2024.csv')
cycle_df = pd.read_csv('CYCLE_2024.csv')
flags_df = pd.read_csv('FLAGS_2024.csv')
person_df = pd.read_csv('PERSON_2024.csv')
roadway_df = pd.read_csv('ROADWAY_2024.csv')
trailveh_df = pd.read_csv('TRAILVEH_2024.csv')

# ---------------------------
# Step 2: Aggregation function with numeric cleaning
# ---------------------------
def aggregate_dataset(df, source_name, numeric_cols=[], categorical_cols=[]):
    """Aggregate per CRN safely, ignoring missing columns"""

    # Only keep columns that exist
    numeric_cols = [col for col in numeric_cols if col in df.columns]
    categorical_cols = [col for col in categorical_cols if col in df.columns]

    # Convert numeric columns to numbers
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    agg_dict = {}
    for col in numeric_cols:
        agg_dict[col] = 'mean'
    for col in categorical_cols:
        agg_dict[col] = lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'

    if len(agg_dict) == 0:
        # If no columns to aggregate, just return unique CRN with SOURCE_TYPE
        agg_df = df[['CRN']].drop_duplicates()
        agg_df['SOURCE_TYPE'] = source_name
        return agg_df

    agg_df = df.groupby('CRN').agg(agg_dict).reset_index()
    agg_df['SOURCE_TYPE'] = source_name
    return agg_df


# ---------------------------
# Step 3: Aggregate all datasets
# ---------------------------
vehicle_agg = aggregate_dataset(vehicle_df, 'Vehicle',
                                numeric_cols=['TRAVEL_SPD','DVR_PRES_IND','PEOPLE_IN_UNIT','COMM_VEH_IND'],
                                categorical_cols=['VEH_TYPE','BODY_TYPE'])

commveh_agg = aggregate_dataset(commveh_df, 'Commercial Vehicle',
                                numeric_cols=['COMM_FEATURE_1','COMM_FEATURE_2'],
                                categorical_cols=['VEH_TYPE'])

cycle_agg = aggregate_dataset(cycle_df, 'Cycle',
                              numeric_cols=['TRAVEL_SPD'],
                              categorical_cols=['VEH_TYPE'])

trailveh_agg = aggregate_dataset(trailveh_df, 'Trail Vehicle',
                                 numeric_cols=['TRAVEL_SPD'],
                                 categorical_cols=['VEH_TYPE'])

person_agg = aggregate_dataset(person_df, 'Person',
                               numeric_cols=['AGE'],
                               categorical_cols=['GENDER'])

flags_agg = aggregate_dataset(flags_df, 'Flags',
                              numeric_cols=[],
                              categorical_cols=['FLAG_TYPE'])

roadway_agg = aggregate_dataset(roadway_df, 'Roadway',
                                numeric_cols=['SPEED_LIMIT'],
                                categorical_cols=['RDWY_ALIGNMENT'])

# ---------------------------
# Step 4: Merge all datasets with crash_df
# ---------------------------
all_agg_dfs = [crash_df, vehicle_agg, commveh_agg, cycle_agg, trailveh_agg,
               person_agg, flags_agg, roadway_agg]

merged_df = reduce(lambda left, right: pd.merge(left, right, on='CRN', how='outer', suffixes=('', '_dup')), all_agg_dfs)

# ---------------------------
# Step 5: Combine all SOURCE_TYPE columns into one
# ---------------------------
source_cols = [col for col in merged_df.columns if 'SOURCE_TYPE' in col]

merged_df['SOURCE_TYPE_COMBINED'] = merged_df[source_cols].apply(lambda x: ', '.join(x.dropna().unique()), axis=1)

# Drop the original individual SOURCE_TYPE columns
merged_df.drop(columns=source_cols, inplace=True)

# ---------------------------
# Step 6: Fill numeric NaNs with 0 (optional)
# ---------------------------
for col in merged_df.select_dtypes(include='number').columns:
    merged_df[col] = merged_df[col].fillna(0)

# ---------------------------
# Step 7: Inspect final dataset
# ---------------------------
print(merged_df.head())
print(merged_df[['CRN','SOURCE_TYPE_COMBINED']].head())

print(merged_df.columns.tolist())

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import folium

# ---------------------------
# Step 1: Load dataset
# ---------------------------

# ---------------------------
# Step 2: Define fixable features
# ---------------------------
fixable_features = [
    'LANE_CLOSED', 'WORK_ZONE_IND', 'WORK_ZONE_LOC', 'WORK_ZONE_TYPE',
    'ILLUMINATION', 'ROADWAY_CLEARED', 'ROAD_CONDITION',
    'TCD_FUNC_CD', 'TCD_TYPE', 'TFC_DETOUR_IND'
]

# Keep only existing columns
fixable_features = [col for col in fixable_features if col in merged_df.columns]

# Target
target = 'MAX_SEVERITY_LEVEL'

X = merged_df[fixable_features].copy()

# ---------------------------
# Step 3: Preprocess features
# ---------------------------
# Fill numeric NaNs with 0
numeric_cols = X.select_dtypes(include='number').columns
X[numeric_cols] = X[numeric_cols].fillna(0)

# Encode categorical features
categorical_cols = X.select_dtypes(include='object').columns
for col in categorical_cols:
    X[col] = X[col].astype(str)
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# ---------------------------
# Step 4: Preprocess target
# ---------------------------
y_raw = pd.to_numeric(merged_df[target], errors='coerce').fillna(0).astype(int)
severity_levels = sorted(y_raw.unique())
severity_map = {v: i for i, v in enumerate(severity_levels)}
y = y_raw.map(severity_map)

# ---------------------------
# Step 5: Train/test split
# ---------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------------------------
# Step 6: Train XGBoost
# ---------------------------
model = xgb.XGBClassifier(
    objective='multi:softprob',
    num_class=len(severity_levels),
    eval_metric='mlogloss',
    use_label_encoder=False
)
model.fit(X_train, y_train)

# ---------------------------
# Step 7: Evaluate
# ---------------------------
y_pred = model.predict(X_test)
inverse_severity_map = {v:k for k,v in severity_map.items()}
y_test_orig = y_test.map(inverse_severity_map)
y_pred_orig = pd.Series(y_pred).map(inverse_severity_map)

print("Classification Report:\n", classification_report(y_test_orig, y_pred_orig))
print("Confusion Matrix:\n", confusion_matrix(y_test_orig, y_pred_orig))

importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values(by='importance', ascending=False)

print("Top fixable features contributing to severity:")
print(importance_df)

# ---------------------------
# Step 9: Create interactive map
# ---------------------------
lat_col = 'DEC_LATITUDE'
lon_col = 'DEC_LONGITUDE'
df_map = merged_df.dropna(subset=[lat_col, lon_col]).copy()
X_map = df_map[fixable_features]

# Encode map features same as training
for col in X_map.select_dtypes(include='object').columns:
    X_map[col] = X_map[col].astype(str)
    le = LabelEncoder()
    X_map[col] = le.fit_transform(X_map[col])

y_proba = model.predict_proba(X_map)

# Pick top 3 contributing fixable features to show on map
top_features = importance_df['feature'].head(3).tolist()

map_center = [df_map[lat_col].mean(), df_map[lon_col].mean()]
m = folium.Map(location=map_center, zoom_start=8)

for idx, row in df_map.iterrows():
    lat = row[lat_col]
    lon = row[lon_col]

    # Severity probabilities
    probs = y_proba[idx]
    prob_str = "<br>".join([f"Severity {inverse_severity_map[i]}: {probs[i]:.2f}"
                            for i in range(len(severity_levels))])

    # Top contributing fixable features
    contrib_features = "<br>".join([f"{feat}: {row[feat]}" for feat in top_features])

    popup_text = f"<b>CRN:</b> {row['CRN']}<br><b>Predicted Severity Probabilities:</b><br>{prob_str}<br><b>Top Fixable Features:</b><br>{contrib_features}"

    folium.CircleMarker(
        location=[lat, lon],
        radius=5,
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.7,
        popup=folium.Popup(popup_text, max_width=300)
    ).add_to(m)

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import BallTree

# Latitude / Longitude columns
lat_col = 'DEC_LATITUDE'
lon_col = 'DEC_LONGITUDE'

# Drop rows without coordinates
df_geo = merged_df.dropna(subset=[lat_col, lon_col]).copy()

# Convert to radians for haversine distance
coords = np.radians(df_geo[[lat_col, lon_col]].values)

# BallTree for spatial nearest-neighbor search
tree = BallTree(coords, metric='haversine')

def predict_crash_risk_at_location(
    latitude,
    longitude,
    k=50
):
    """
    Predict crash severity probabilities and top causes
    at a given latitude/longitude in Pennsylvania.
    """

    # ---------------------------
    # 1. Find nearest crashes
    # ---------------------------
    query_point = np.radians([[latitude, longitude]])
    dist, idx = tree.query(query_point, k=k)

    nearby = df_geo.iloc[idx[0]].copy()

    # ---------------------------
    # 2. Aggregate fixable features
    # ---------------------------
    X_local = {}

    for col in fixable_features:
        if nearby[col].dtype == 'object':
            X_local[col] = nearby[col].mode().iloc[0]
        else:
            X_local[col] = nearby[col].mean()

    X_local = pd.DataFrame([X_local])

    # ---------------------------
    # 3. Encode categoricals exactly like training
    # ---------------------------
    for col in X_local.select_dtypes(include='object').columns:
        le = LabelEncoder()
        le.fit(merged_df[col].astype(str))
        X_local[col] = le.transform(X_local[col].astype(str))

    # ---------------------------
    # 4. Predict severity probabilities
    # ---------------------------
    probs = model.predict_proba(X_local)[0]

    severity_probs = {
        f"P(Severity = {inverse_severity_map[i]})": round(probs[i], 4)
        for i in range(len(probs))
    }

    # ---------------------------
    # 5. Top contributing causes
    # ---------------------------
    importance_df_sorted = importance_df.copy()

    importance_df_sorted['local_value'] = importance_df_sorted['feature'].map(
        lambda f: X_local.iloc[0][f]
    )

    top_causes = importance_df_sorted.head(5)[
        ['feature', 'importance', 'local_value']
    ]

    return severity_probs, top_causes

lat = 42.4406   # Pittsburgh
lon = -81.9959

severity_probs, top_causes = predict_crash_risk_at_location(lat, lon)

print("Severity Probabilities:")
for k, v in severity_probs.items():
    print(f"{k}: {v}")

print("\nTop Crash Causes (with relevance):")
print(top_causes)

import pandas as pd
import numpy as np
import shap
from sklearn.metrics import pairwise_distances_argmin_min

def predict_crash_risk_at_location(lat, lon, df_map, X_map, model, severity_levels, inverse_severity_map, top_n=3):
    """
    Predict severity probabilities and top crash causes at a given location.

    Returns:
        severity_probs: dict {severity_name: probability}
        top_causes: list of dicts [{feature, feature_value, shap_value}]
    """
    # 1️⃣ Find the closest crash
    coords = df_map[['DEC_LATITUDE', 'DEC_LONGITUDE']].values
    closest_idx, _ = pairwise_distances_argmin_min(np.array([[lat, lon]]), coords)
    closest_idx = closest_idx[0]

    # 2️⃣ Extract features for this crash
    row_features = X_map.iloc[closest_idx:closest_idx+1]

    # 3️⃣ Predict probabilities
    probs = model.predict_proba(row_features)[0]
    severity_probs = {inverse_severity_map[i]: probs[i] for i in range(len(severity_levels))}

    # 4️⃣ Compute SHAP values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(row_features)

    # 5️⃣ Pick the relevant class SHAP (highest predicted probability)
    if isinstance(shap_values, list):  # multi-class
        top_class = probs.argmax()
        row_shap = np.array(shap_values[top_class][0], dtype=float).flatten()
    else:
        row_shap = np.array(shap_values[0], dtype=float).flatten()

    # 6️⃣ Build DataFrame safely
    features = []
    shap_vals = []
    values = []

    for i, col in enumerate(X_map.columns):
        features.append(col)
        shap_vals.append(float(row_shap[i]))  # ensure scalar
        val = row_features.iloc[0, i]
        if isinstance(val, (np.ndarray, list)):
            val = val.item()  # convert single-element array/list to scalar
        values.append(val)

    row_shap_df = pd.DataFrame({
        'feature': features,
        'shap_value': shap_vals,
        'feature_value': values
    })

    # 7️⃣ Select top_n by absolute SHAP
    top_row_features = row_shap_df.reindex(
        row_shap_df['shap_value'].abs().sort_values(ascending=False).index
    ).head(top_n)

    # 8️⃣ Format results
    top_causes = []
    for _, r in top_row_features.iterrows():
        top_causes.append({
            'feature': r['feature'],
            'feature_value': r['feature_value'],
            'shap_value': r['shap_value']
        })

    return severity_probs, top_causes

lat = 41.385   # Pittsburgh
lon = -75.608

severity_probs, top_causes = predict_crash_risk_at_location(
    lat, lon, df_map, X_map, model, severity_levels, inverse_severity_map, top_n=3
)

print("Severity Probabilities:")
for k, v in severity_probs.items():
    print(f"{k}: {v:.3f}")

print("\nTop Crash Causes (with relevance):")
for cause in top_causes:
    print(f"{cause['feature']}: value={cause['feature_value']}, SHAP={cause['shap_value']:.3f}")